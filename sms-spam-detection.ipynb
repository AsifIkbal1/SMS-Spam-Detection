{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/mohamedchahed/ML-projects/blob/main/Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"","metadata":{"id":"GBLoF6m6hiXV"}},{"cell_type":"markdown","source":"# ðŸ“– Background \n","metadata":{"id":"7-uSnxHxhyUb"}},{"cell_type":"markdown","source":"As we navigate through the digital world, spam messages have become an increasingly common and annoying problem. In order to address this issue, I am working on build a spam classifier. The goal of this project is to develop a model that can accurately distinguish between legitimate (ham) messages and unwanted (spam) messages in a dataset of SMS messages.","metadata":{"id":"-1AXar2hiF8T"}},{"cell_type":"markdown","source":"# ðŸ’¾ Data\n","metadata":{"id":"0CU9OhOwkejc"}},{"cell_type":"markdown","source":"* The provided dataset consists of more than 5,500 SMS messages in English, out of which approximately 13% have been classified as spam. Each message is stored in a text file, with each line containing two columns - the message label (either \"ham\" or \"spam\") and the original text of the message. Messages that have been labeled as \"ham\" are considered to be non-spam messages that are authentic\n\n\n* The dataset can be accessed through the following [link](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)\n\n","metadata":{"id":"n2NQRCDFkkjn"}},{"cell_type":"code","source":"# Load the dataframe \nimport pandas as pd \ndf = pd.read_csv(\"/spam.csv\")\ndf = df.rename(columns={'0': 'label', '1': 'text'})\ndf ","metadata":{"id":"NOhsx-oOmNfA","outputId":"356af408-b198-4bda-c506-dd1b731d7f38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ›  Data Preprocessing ","metadata":{"id":"_HBDPm-noKiK"}},{"cell_type":"markdown","source":"### Target Encoding ","metadata":{"id":"GjINMb_4zmQ8"}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df['label'])\n\ndf['label'] = le.transform(df['label'])\ndf.head()","metadata":{"id":"mOgjStibzmrZ","outputId":"cbfdb182-4e60-488a-b44c-0eb2e3932f65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Corpus Cleaning","metadata":{"id":"NPFvkvuqokUx"}},{"cell_type":"markdown","source":"Text cleaning is an essential preprocessing step for a spam classifier. The aim is to remove any unnecessary elements from the text data, such as punctuation, special characters, and stopwords. This helps to create a more standardized dataset and removes inconsistencies in the text that could affect model accuracy. \n\nAdditionally, converting all text to lowercase ensures consistency across the data, regardless of capitalization differences.","metadata":{"id":"LdFaiZYUqwry"}},{"cell_type":"markdown","source":"","metadata":{"id":"AxnDnbQ1uuZn"}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"id":"3mB2YnuHvUHP","outputId":"120a943d-1572-469a-ae45-b911211892ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):\n  \n    # Make text lowercase\n    text = text.lower()\n\n    # Remove text in square brackets\n    text = re.sub('\\[.*?\\]', '', text)\n\n    # Remove links\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n\n    # Remove punctuation\n    text = re.sub('[^a-zA-Z0-9\\s]+', '', text)\n\n    # Remove words containing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    text = ' '.join(filtered_words)\n\n    # Remove extra whitespace\n    text = re.sub('\\s+', ' ', text).strip()\n\n    return text","metadata":{"id":"-U8UiuuRrNya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the clean_text function to all text in the 'text' column\ndf['text'] = df['text'].apply(clean_text)\n\n# Show the updated dataframe\ndf.head()","metadata":{"id":"71HpHqR5sVaX","outputId":"9df402d3-e5b7-413b-9206-865b484edb8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stemming","metadata":{"id":"nbajZsLYvcGf"}},{"cell_type":"markdown","source":"Stemming is the process of reducing words to their base or root form, with the goal of reducing inflectional and derivational variations. The resulting base form, or stem, may not be a valid word on its own, but it can be used to group together different variations of a word.\n\n> There are two types of stemming :\n* Snowball Stemming \n* Porter Stemming \n\nBoth Snowball stemming and Porter stemming can be suitable for spam classification.\n\nHowever, since the goal of a spam classifier is to maximize recall and minimize the number of false negatives, it may be more appropriate to use a more aggressive stemming algorithm like Snowball stemming.\n","metadata":{"id":"NmuPF4efwXAD"}},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer\n\n# initialize SnowballStemmer \nstemmer = SnowballStemmer('english')\n\ndef stem_text(text):\n    # Tokenize the input text into individual words\n    tokens = nltk.word_tokenize(text)\n    \n    # Stem each token using the SnowballStemmer\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    \n    # Join the stemmed tokens back into a single string\n    return ' '.join(stemmed_tokens)","metadata":{"id":"GBwvynrdxQn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"id":"FvH1EVpfzLzD","outputId":"06a9ec7e-5ca2-4893-fb20-f0294067b6b4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply stemming to the 'text' column in your DataFrame\ndf['text'] = df['text'].apply(stem_text)\n# Show the updated dataframe\ndf.head()","metadata":{"id":"hetho3aSzDiE","outputId":"5bfdc779-7e52-425a-e250-1654c3f41a04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“Š Modeling \n\n\n\n","metadata":{"id":"08mDhd5z1b60"}},{"cell_type":"markdown","source":"## Classical ML Models","metadata":{"id":"-agyrdqQ1xK0"}},{"cell_type":"markdown","source":"In this section, we will be exploring the use of classical machine learning models with the TF-IDF vectorization technique as a starting point. While we recognize that more complex models, such as deep learning models, have been shown to achieve state-of-the-art performance in many natural language processing tasks, we believe it is important to first establish a strong baseline with simpler models before exploring more complex approaches.","metadata":{"id":"IaJWzouH3f4X"}},{"cell_type":"markdown","source":"### TF-IDF vectorization","metadata":{"id":"X6zEDLDd4Y6W"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Define TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Vectorize the text data\nX = vectorizer.fit_transform(df['text'])\n\n# Define target variable\ny = df['label']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"id":"nurCSrzU2cFy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building","metadata":{"id":"sLgPCxl_6YBZ"}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_val_score\n\n# Fit different models\nnb_model = MultinomialNB().fit(X_train, y_train)\nsvm_model = LinearSVC().fit(X_train, y_train)\nrf_model = RandomForestClassifier().fit(X_train, y_train)\ngb_model = GradientBoostingClassifier().fit(X_train, y_train)","metadata":{"id":"FK0bQ--465q2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{"id":"YV_ofYaG6ZpP"}},{"cell_type":"code","source":"import seaborn as sns \n# Define a function that takes a model, X, and y and returns the accuracy, precision, recall, and F1 score using 10-fold cross-validation.\ndef evaluate_model(model, X, y):\n    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    precision = cross_val_score(model, X, y, cv=5, scoring='precision_macro')\n    recall = cross_val_score(model, X, y, cv=5, scoring='recall_macro')\n    f1 = cross_val_score(model, X, y, cv=5, scoring='f1_macro')\n    return {'accuracy': accuracy.mean(), 'precision': precision.mean(), 'recall': recall.mean(), 'f1': f1.mean()}\n# A dictionary that maps each model to its name\nmodels = {'Multinomial Naive Bayes': nb_model, 'Support Vector Machine': svm_model, 'Random Forest': rf_model, 'Gradient Boosting': gb_model}\n# Create an empty dictionary to store the evaluation results\nresults = {}\n# Loop through models and evaluate\nfor name, model in models.items():\n    results[name] = evaluate_model(model, X_train, y_train)\n# Create a DataFrame from the results dictionary.\nresults_df = pd.DataFrame.from_dict(results, orient='index')\nsns.heatmap(results_df, cmap='Blues', annot=True)\n","metadata":{"id":"qW64YDaF-0jC","outputId":"10ab9b66-72c3-4fd6-d36b-00a861a77c91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep Learning ","metadata":{"id":"QTDhfSqoC__6"}},{"cell_type":"markdown","source":"In this section, we will be exploring more advanced models to improve our results. Specifically, we will be leveraging the power of Long Short-Term Memory (LSTM) models and Global Vectors for Word Representation (GloVe) word embeddings. These techniques have proven to be highly effective in NLP tasks. We will also compare our results to those achieved using transfer learning with Bidirectional Encoder Representations from Transformers (BERT), a state-of-the-art language model. By utilizing these sophisticated models and techniques, we hope to achieve even greater accuracy and precision in our predictions.","metadata":{"id":"-MQJb48aWiqR"}},{"cell_type":"markdown","source":"### Word embeddings with GloVe","metadata":{"id":"sFY5d8naGN9g"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport gensim.downloader as api\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Bidirectional, Dropout, GlobalMaxPool1D, BatchNormalization","metadata":{"id":"mhOLP9q7XV7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n\n# Convert the text data into sequences of integer values\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Pad the sequences to ensure they all have the same length\nmaxlen = 100\nX_train_padded = pad_sequences(X_train_sequences, padding='post', truncating='post', maxlen=maxlen)\nX_test_padded = pad_sequences(X_test_sequences, padding='post', truncating='post', maxlen=maxlen)\n\n# Load the pre-trained GloVe embeddings\nword_vectors = api.load('glove-wiki-gigaword-100')\n\n# Create the embedding layer\nembedding_dim = 100\nvocab_size = len(tokenizer.word_index) + 1\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if word in word_vectors:\n        embedding_vector = word_vectors[word]\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\nembedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)","metadata":{"id":"mR0YeGKxFHG6","outputId":"5f4f04eb-e56b-47bf-ebc6-034f21f23a09"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM Model ","metadata":{"id":"hUKOZBy6Rc5X"}},{"cell_type":"code","source":"# Define the LSTM model\nmodel = Sequential()\nmodel.add(embedding_layer)\n    \nmodel.add(Bidirectional(LSTM(\n        100, \n        return_sequences = True, \n        recurrent_dropout=0.2)))  \nmodel.add(GlobalMaxPool1D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))","metadata":{"id":"SKHSMXyQRCPa","outputId":"2f3884e8-3695-4721-f510-45753a9739af"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While I was able to achieve a satisfying level of accuracy after training the model for just 5 epochs, it's worth noting that there is potential for further improvement. By increasing the number of epochs, the model would potentially achieve even higher accuracy. Additionally, experimenting with different model architectures may also lead to better performance.\n\n","metadata":{"id":"14lKDWjWW1Xw"}},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{"id":"cnrJuknXSzeI"}},{"cell_type":"code","source":"y_pred = model.predict(X_test_padded)\ny_pred_rounded = np.round(y_pred)\n\naccuracy = accuracy_score(y_test, y_pred_rounded)\nprecision = precision_score(y_test, y_pred_rounded)\nrecall = recall_score(y_test, y_pred_rounded)\nf1 = f1_score(y_test, y_pred_rounded)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 score:\", f1) \n","metadata":{"id":"y4rmOKQUSh0u","outputId":"991137f8-d722-47dd-d09a-6b1ab9ab5fee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nimport matplotlib.pyplot as plt\n \n# Generate confusion matrix\ncm = confusion_matrix(y_test, y_pred_rounded)\n \n# Plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"id":"ZRnleyujTLUs","outputId":"b03bb1f5-2360-4893-bc4e-a79a72da437c"},"execution_count":null,"outputs":[]}]}